{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59bfaab",
   "metadata": {},
   "source": [
    "# CNN Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ec0efdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, activations, regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "80792f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinbell/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/keras/src/datasets/cifar.py:18: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  d = cPickle.load(f, encoding=\"bytes\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (50000, 32, 32, 3)\n",
      "y_train shape:  (50000, 10)\n",
      "X_test shape:  (10000, 32, 32, 3)\n",
      "y_test shape:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation and Processing\n",
    "data = cifar10.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "#Target transformation\n",
    "output_encoder = LabelBinarizer()\n",
    "y_train = output_encoder.fit_transform(y_train)\n",
    "y_test = output_encoder.transform(y_test)\n",
    "\n",
    " # Shapes of data variables\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5f8de920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(output_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "12a76546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "10a4a9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.60392157 0.69411765 0.73333333]\n",
      "  [0.49411765 0.5372549  0.53333333]\n",
      "  [0.41176471 0.40784314 0.37254902]\n",
      "  ...\n",
      "  [0.35686275 0.37254902 0.27843137]\n",
      "  [0.34117647 0.35294118 0.27843137]\n",
      "  [0.30980392 0.31764706 0.2745098 ]]\n",
      "\n",
      " [[0.54901961 0.62745098 0.6627451 ]\n",
      "  [0.56862745 0.6        0.60392157]\n",
      "  [0.49019608 0.49019608 0.4627451 ]\n",
      "  ...\n",
      "  [0.37647059 0.38823529 0.30588235]\n",
      "  [0.30196078 0.31372549 0.24313725]\n",
      "  [0.27843137 0.28627451 0.23921569]]\n",
      "\n",
      " [[0.54901961 0.60784314 0.64313725]\n",
      "  [0.54509804 0.57254902 0.58431373]\n",
      "  [0.45098039 0.45098039 0.43921569]\n",
      "  ...\n",
      "  [0.30980392 0.32156863 0.25098039]\n",
      "  [0.26666667 0.2745098  0.21568627]\n",
      "  [0.2627451  0.27058824 0.21568627]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.68627451 0.65490196 0.65098039]\n",
      "  [0.61176471 0.60392157 0.62745098]\n",
      "  [0.60392157 0.62745098 0.66666667]\n",
      "  ...\n",
      "  [0.16470588 0.13333333 0.14117647]\n",
      "  [0.23921569 0.20784314 0.22352941]\n",
      "  [0.36470588 0.3254902  0.35686275]]\n",
      "\n",
      " [[0.64705882 0.60392157 0.50196078]\n",
      "  [0.61176471 0.59607843 0.50980392]\n",
      "  [0.62352941 0.63137255 0.55686275]\n",
      "  ...\n",
      "  [0.40392157 0.36470588 0.37647059]\n",
      "  [0.48235294 0.44705882 0.47058824]\n",
      "  [0.51372549 0.4745098  0.51372549]]\n",
      "\n",
      " [[0.63921569 0.58039216 0.47058824]\n",
      "  [0.61960784 0.58039216 0.47843137]\n",
      "  [0.63921569 0.61176471 0.52156863]\n",
      "  ...\n",
      "  [0.56078431 0.52156863 0.54509804]\n",
      "  [0.56078431 0.5254902  0.55686275]\n",
      "  [0.56078431 0.52156863 0.56470588]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d6aec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "checkpoint = ModelCheckpoint(f\"conv_weights/Conv2D Model/{epochs}e-{batch_size}bs\" + \"-{loss:.4f}\" + \".keras\", monitor='loss', verbose=1,\n",
    "                                         save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cc0cee65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_47\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_47\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_134 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_135 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">75,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_136 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">120,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_134 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_70 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_135 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m48\u001b[0m)       │        \u001b[38;5;34m75,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_71 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m48\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_136 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m100\u001b[0m)      │       \u001b[38;5;34m120,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_47 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_99 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m160,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_100 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">358,954</span> (1.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m358,954\u001b[0m (1.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">358,954</span> (1.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m358,954\u001b[0m (1.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#Model Creation\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(5,5), activation=activations.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    Conv2D(filters=48, kernel_size=(7,7), activation=activations.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    Conv2D(filters=100, kernel_size=(5,5), padding='same', activation=activations.relu, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Flatten(),\n",
    "    Dense(units=100, activation=activations.relu),\n",
    "    # Dropout(0.5),\n",
    "    Dense(units=10, activation=activations.softmax)\n",
    "])\n",
    "model.build(input_shape=(None,) + X_train.shape[1:4])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "58f4f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m448/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.2452 - loss: 2.2242\n",
      "Epoch 1: loss improved from None to 1.93264, saving model to conv_weights/Conv2D Model/20e-100bs-1.9326.keras\n",
      "\n",
      "Epoch 1: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.9326.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.3328 - loss: 1.9326 - val_accuracy: 0.4230 - val_loss: 1.6898\n",
      "Epoch 2/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4511 - loss: 1.6185\n",
      "Epoch 2: loss improved from 1.93264 to 1.59774, saving model to conv_weights/Conv2D Model/20e-100bs-1.5977.keras\n",
      "\n",
      "Epoch 2: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.5977.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.4608 - loss: 1.5977 - val_accuracy: 0.4842 - val_loss: 1.5635\n",
      "Epoch 3/20\n",
      "\u001b[1m449/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4940 - loss: 1.5149\n",
      "Epoch 3: loss improved from 1.59774 to 1.50491, saving model to conv_weights/Conv2D Model/20e-100bs-1.5049.keras\n",
      "\n",
      "Epoch 3: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.5049.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.5001 - loss: 1.5049 - val_accuracy: 0.5330 - val_loss: 1.4475\n",
      "Epoch 4/20\n",
      "\u001b[1m448/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5301 - loss: 1.4468\n",
      "Epoch 4: loss improved from 1.50491 to 1.44580, saving model to conv_weights/Conv2D Model/20e-100bs-1.4458.keras\n",
      "\n",
      "Epoch 4: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.4458.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.5296 - loss: 1.4458 - val_accuracy: 0.5486 - val_loss: 1.4067\n",
      "Epoch 5/20\n",
      "\u001b[1m449/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5482 - loss: 1.3996\n",
      "Epoch 5: loss improved from 1.44580 to 1.39827, saving model to conv_weights/Conv2D Model/20e-100bs-1.3983.keras\n",
      "\n",
      "Epoch 5: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.3983.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - accuracy: 0.5502 - loss: 1.3983 - val_accuracy: 0.5614 - val_loss: 1.3649\n",
      "Epoch 6/20\n",
      "\u001b[1m449/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5675 - loss: 1.3576\n",
      "Epoch 6: loss improved from 1.39827 to 1.35873, saving model to conv_weights/Conv2D Model/20e-100bs-1.3587.keras\n",
      "\n",
      "Epoch 6: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.3587.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.5678 - loss: 1.3587 - val_accuracy: 0.5698 - val_loss: 1.3468\n",
      "Epoch 7/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5812 - loss: 1.3242\n",
      "Epoch 7: loss improved from 1.35873 to 1.32462, saving model to conv_weights/Conv2D Model/20e-100bs-1.3246.keras\n",
      "\n",
      "Epoch 7: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.3246.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.5828 - loss: 1.3246 - val_accuracy: 0.5934 - val_loss: 1.3095\n",
      "Epoch 8/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5954 - loss: 1.2932\n",
      "Epoch 8: loss improved from 1.32462 to 1.29636, saving model to conv_weights/Conv2D Model/20e-100bs-1.2964.keras\n",
      "\n",
      "Epoch 8: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.2964.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.5956 - loss: 1.2964 - val_accuracy: 0.6122 - val_loss: 1.2843\n",
      "Epoch 9/20\n",
      "\u001b[1m449/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6045 - loss: 1.2732\n",
      "Epoch 9: loss improved from 1.29636 to 1.27576, saving model to conv_weights/Conv2D Model/20e-100bs-1.2758.keras\n",
      "\n",
      "Epoch 9: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.2758.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.6048 - loss: 1.2758 - val_accuracy: 0.6206 - val_loss: 1.2646\n",
      "Epoch 10/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6137 - loss: 1.2541\n",
      "Epoch 10: loss improved from 1.27576 to 1.25643, saving model to conv_weights/Conv2D Model/20e-100bs-1.2564.keras\n",
      "\n",
      "Epoch 10: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.2564.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.6136 - loss: 1.2564 - val_accuracy: 0.6288 - val_loss: 1.2477\n",
      "Epoch 11/20\n",
      "\u001b[1m449/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6208 - loss: 1.2393\n",
      "Epoch 11: loss improved from 1.25643 to 1.24327, saving model to conv_weights/Conv2D Model/20e-100bs-1.2433.keras\n",
      "\n",
      "Epoch 11: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.2433.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.6205 - loss: 1.2433 - val_accuracy: 0.6358 - val_loss: 1.2314\n",
      "Epoch 12/20\n",
      "\u001b[1m448/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6257 - loss: 1.2265\n",
      "Epoch 12: loss improved from 1.24327 to 1.23031, saving model to conv_weights/Conv2D Model/20e-100bs-1.2303.keras\n",
      "\n",
      "Epoch 12: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.2303.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.6260 - loss: 1.2303 - val_accuracy: 0.6420 - val_loss: 1.2162\n",
      "Epoch 13/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6305 - loss: 1.2167\n",
      "Epoch 13: loss improved from 1.23031 to 1.21822, saving model to conv_weights/Conv2D Model/20e-100bs-1.2182.keras\n",
      "\n",
      "Epoch 13: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.2182.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.6320 - loss: 1.2182 - val_accuracy: 0.6402 - val_loss: 1.2162\n",
      "Epoch 14/20\n",
      "\u001b[1m448/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6406 - loss: 1.1992\n",
      "Epoch 14: loss improved from 1.21822 to 1.20326, saving model to conv_weights/Conv2D Model/20e-100bs-1.2033.keras\n",
      "\n",
      "Epoch 14: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.2033.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.6400 - loss: 1.2033 - val_accuracy: 0.6434 - val_loss: 1.2131\n",
      "Epoch 15/20\n",
      "\u001b[1m448/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6464 - loss: 1.1884\n",
      "Epoch 15: loss improved from 1.20326 to 1.18950, saving model to conv_weights/Conv2D Model/20e-100bs-1.1895.keras\n",
      "\n",
      "Epoch 15: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.1895.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.6470 - loss: 1.1895 - val_accuracy: 0.6490 - val_loss: 1.2002\n",
      "Epoch 16/20\n",
      "\u001b[1m449/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6484 - loss: 1.1857\n",
      "Epoch 16: loss improved from 1.18950 to 1.18700, saving model to conv_weights/Conv2D Model/20e-100bs-1.1870.keras\n",
      "\n",
      "Epoch 16: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.1870.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.6482 - loss: 1.1870 - val_accuracy: 0.6420 - val_loss: 1.2160\n",
      "Epoch 17/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6520 - loss: 1.1746\n",
      "Epoch 17: loss improved from 1.18700 to 1.17636, saving model to conv_weights/Conv2D Model/20e-100bs-1.1764.keras\n",
      "\n",
      "Epoch 17: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.1764.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.6530 - loss: 1.1764 - val_accuracy: 0.6450 - val_loss: 1.2099\n",
      "Epoch 18/20\n",
      "\u001b[1m449/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6559 - loss: 1.1689\n",
      "Epoch 18: loss improved from 1.17636 to 1.17018, saving model to conv_weights/Conv2D Model/20e-100bs-1.1702.keras\n",
      "\n",
      "Epoch 18: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.1702.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.6558 - loss: 1.1702 - val_accuracy: 0.6498 - val_loss: 1.1837\n",
      "Epoch 19/20\n",
      "\u001b[1m449/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6605 - loss: 1.1547\n",
      "Epoch 19: loss improved from 1.17018 to 1.15683, saving model to conv_weights/Conv2D Model/20e-100bs-1.1568.keras\n",
      "\n",
      "Epoch 19: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.1568.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.6611 - loss: 1.1568 - val_accuracy: 0.6480 - val_loss: 1.1892\n",
      "Epoch 20/20\n",
      "\u001b[1m449/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6657 - loss: 1.1469\n",
      "Epoch 20: loss improved from 1.15683 to 1.14809, saving model to conv_weights/Conv2D Model/20e-100bs-1.1481.keras\n",
      "\n",
      "Epoch 20: finished saving model to conv_weights/Conv2D Model/20e-100bs-1.1481.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - accuracy: 0.6654 - loss: 1.1481 - val_accuracy: 0.6490 - val_loss: 1.1870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x126d6cb90>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "# model.load_weights(\"conv_weights/Conv2D Model/20e-100bs-0.9729.keras\")\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, shuffle=False, callbacks=callbacks, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_train[:10])\n",
    "pred_decode = output_encoder.inverse_transform(predictions)\n",
    "print(pred_decode)\n",
    "print(output_encoder.inverse_transform(y_train[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/0_/nmh5w64x3q5_ffwfkr35sscw0000gn/T/tmphi4ywg6h'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='keras_tensor_43')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  4895227984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895229136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895229712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895229904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895230096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895230480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895230288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895230864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895230672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895231248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1766688857.598078  212361 tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "W0000 00:00:1766688857.598484  212361 tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "I0000 00:00:1766688857.608609  212361 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model) # or .from_saved_model(saved_model_dir)\n",
    "# tflite_model = converter.convert()\n",
    "# with open('model.tflite', 'wb') as f:\n",
    "#     f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbfe5d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a .h5 file\n",
    "model.save(\"object_classifier_model4.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b50ea4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_names = ['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f76c8b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow version 2.20.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767062541.593652  261481 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1767062541.595610  261481 single_machine.cc:376] Starting new session\n",
      "I0000 00:00:1767062541.630193  261481 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1767062541.630246  261481 single_machine.cc:376] Starting new session\n",
      "I0000 00:00:1767062541.661509  261481 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1767062541.661555  261481 single_machine.cc:376] Starting new session\n",
      "I0000 00:00:1767062541.681517  261481 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1767062541.681568  261481 single_machine.cc:376] Starting new session\n",
      "Running TensorFlow Graph Passes:   0%|          | 0/6 [00:00<?, ? passes/s]I0000 00:00:1767062542.122092  261481 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 27.86 passes/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input (input_image) provided is not found in given tensorflow graph. Placeholders in graph are: ['keras_tensor_402']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# # Convert the model\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# mlmodel = ct.convert(\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#     'object_classifier_model4.h5',\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#     source=\"tensorflow\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     12\u001b[39m image_input = ct.converters.mil.input_types.ImageType(\n\u001b[32m     13\u001b[39m     name=\u001b[33m'\u001b[39m\u001b[33minput_image\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     14\u001b[39m     shape=(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m), \n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     color_layout=\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m mlmodel = \u001b[43mct\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                     \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                     \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmlprogram\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mminimum_deployment_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mct\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43miOS16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# mlmodel = ct.convert(model=model, \u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#                      inputs=[ct.TensorType(dtype=np.float16)], \u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m#                      source=\"tensorflow\", convert_to=\"mlprogram\", \u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#                      minimum_deployment_target=ct.target.iOS16,\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#                      output_names=['output'])\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Save the converted model\u001b[39;00m\n\u001b[32m     31\u001b[39m mlmodel.save(\u001b[33m\"\u001b[39m\u001b[33mObjectClassifierModel.mlpackage\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/_converters_entry.py:646\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline, states)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(states) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m exact_source != \u001b[33m\"\u001b[39m\u001b[33mpytorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mstates\u001b[39m\u001b[33m'\u001b[39m\u001b[33m can only be passed with pytorch source model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m mlmodel = \u001b[43mmil_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexact_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexact_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs_as_tensor_or_image_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# None or list[ct.ImageType/ct.TensorType]\u001b[39;49;00m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclassifier_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclassifier_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_model_load\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_model_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpackage_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpackage_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecification_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspecification_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmain_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpass_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_default_fp16_io\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_default_fp16_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exact_target == \u001b[33m\"\u001b[39m\u001b[33mmlprogram\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mlmodel._input_has_infinite_upper_bound():\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    665\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFor mlprogram, inputs with infinite upper_bound is not allowed. Please set upper_bound\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    666\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m to a positive value in \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRangeDim()\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m for the \u001b[39m\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m param in ct.convert().\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    667\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/converter.py:186\u001b[39m, in \u001b[36mmil_convert\u001b[39m\u001b[34m(model, convert_from, convert_to, compute_units, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;129m@_profile\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmil_convert\u001b[39m(\n\u001b[32m    149\u001b[39m     model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     **kwargs\n\u001b[32m    154\u001b[39m ):\n\u001b[32m    155\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[33;03m    Convert model from a specified frontend `convert_from` to a specified\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03m    converter backend `convert_to`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m \u001b[33;03m        See `coremltools.converters.convert`\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mil_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mConverterRegistry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mct\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMLModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/converter.py:218\u001b[39m, in \u001b[36m_mil_convert\u001b[39m\u001b[34m(model, convert_from, convert_to, registry, modelClass, compute_units, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m     weights_dir = _tempfile.TemporaryDirectory()\n\u001b[32m    216\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mweights_dir\u001b[39m\u001b[33m\"\u001b[39m] = weights_dir.name\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m proto, mil_program = \u001b[43mmil_convert_to_proto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m _reset_conversion_state()\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to == \u001b[33m'\u001b[39m\u001b[33mmilinternal\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/converter.py:294\u001b[39m, in \u001b[36mmil_convert_to_proto\u001b[39m\u001b[34m(model, convert_from, convert_to, converter_registry, main_pipeline, **kwargs)\u001b[39m\n\u001b[32m    289\u001b[39m frontend_pipeline, backend_pipeline = _construct_other_pipelines(\n\u001b[32m    290\u001b[39m     main_pipeline, convert_from, convert_to\n\u001b[32m    291\u001b[39m )\n\u001b[32m    293\u001b[39m frontend_converter = frontend_converter_type()\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m prog = \u001b[43mfrontend_converter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m PassPipelineManager.apply_pipeline(prog, frontend_pipeline)\n\u001b[32m    297\u001b[39m PassPipelineManager.apply_pipeline(prog, main_pipeline)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/converter.py:96\u001b[39m, in \u001b[36mTensorFlow2Frontend.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfrontend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensorflow2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TF2Loader\n\u001b[32m     95\u001b[39m tf2_loader = TF2Loader(*args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf2_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/frontend/tensorflow/load.py:82\u001b[39m, in \u001b[36mTFLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m     dot_string = \u001b[38;5;28mself\u001b[39m._tf_ssa.get_dot_string(\n\u001b[32m     76\u001b[39m         annotation=\u001b[38;5;28;01mTrue\u001b[39;00m, name_and_op_style=\u001b[38;5;28;01mTrue\u001b[39;00m, highlight_debug_nodes=[]\n\u001b[32m     77\u001b[39m     )\n\u001b[32m     78\u001b[39m     graphviz.Source(dot_string).view(\n\u001b[32m     79\u001b[39m         filename=\u001b[33m\"\u001b[39m\u001b[33m/tmp/ssa_before_tf_passes\u001b[39m\u001b[33m\"\u001b[39m, cleanup=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     80\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m program = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_program_from_tf_ssa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mprogram:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(program))\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m program\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/frontend/tensorflow2/load.py:203\u001b[39m, in \u001b[36mTF2Loader._program_from_tf_ssa\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_program_from_tf_ssa\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    202\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_tf_ssa_passes()\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     converter = \u001b[43mTF2Converter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtfssa\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tf_ssa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspecification_version\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_default_fp16_io\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_default_fp16_io\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m converter.convert()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/frontend/tensorflow/converter.py:196\u001b[39m, in \u001b[36mTFConverter.__init__\u001b[39m\u001b[34m(self, tfssa, inputs, outputs, opset_version, use_default_fp16_io)\u001b[39m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    193\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMultiple inputs are found in graph, but no input name was provided\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    194\u001b[39m     )\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inp.name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tf_placeholder_names:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInput (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m) provided is not found in given tensorflow graph. Placeholders in graph are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    198\u001b[39m             inp.name, tf_placeholder_names\n\u001b[32m    199\u001b[39m         )\n\u001b[32m    200\u001b[39m     )\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# We fill in shapes and dtypes for user-specified input that doesn't set\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inp.shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Input (input_image) provided is not found in given tensorflow graph. Placeholders in graph are: ['keras_tensor_402']"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "\n",
    "# # Convert the model\n",
    "# mlmodel = ct.convert(\n",
    "#     'object_classifier_model4.h5',\n",
    "#     convert_to=\"mlprogram\", # Use mlprogram for better performance on newer iOS versions\n",
    "#     minimum_deployment_target=ct.target.iOS15, # Specify your minimum iOS target\n",
    "#     source=\"tensorflow\"\n",
    "# )\n",
    "\n",
    "image_input = ct.converters.mil.input_types.ImageType(\n",
    "    name='input_image', \n",
    "    shape=(1, 3, 224, 224), \n",
    "    scale=1.0/255.0, \n",
    "    bias=[0.0, 0.0, 0.0], \n",
    "    color_layout='RGB'\n",
    ")\n",
    "\n",
    "mlmodel = ct.converters.convert(model=model, \n",
    "                     inputs=[image_input], \n",
    "                     source=\"auto\", convert_to=\"mlprogram\", \n",
    "                     minimum_deployment_target=ct.target.iOS16)\n",
    "\n",
    "# mlmodel = ct.convert(model=model, \n",
    "#                      inputs=[ct.TensorType(dtype=np.float16)], \n",
    "#                      source=\"tensorflow\", convert_to=\"mlprogram\", \n",
    "#                      minimum_deployment_target=ct.target.iOS16,\n",
    "#                      output_names=['output'])\n",
    "# Save the converted model\n",
    "mlmodel.save(\"ObjectClassifierModel.mlpackage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
