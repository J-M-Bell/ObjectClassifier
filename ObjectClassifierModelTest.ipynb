{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59bfaab",
   "metadata": {},
   "source": [
    "# CNN Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ec0efdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# from keras import Input, activations, regularizers\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "# from keras.losses import CategoricalCrossentropy\n",
    "# from keras.models import Sequential\n",
    "# from keras.datasets import cifar10\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "80792f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (50000, 32, 32, 3)\n",
      "y_train shape:  (50000, 10)\n",
      "X_test shape:  (10000, 32, 32, 3)\n",
      "y_test shape:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation and Processing\n",
    "data = tf.keras.datasets.cifar10.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "#Target transformation\n",
    "output_encoder = LabelBinarizer()\n",
    "y_train = output_encoder.fit_transform(y_train)\n",
    "y_test = output_encoder.transform(y_test)\n",
    "\n",
    " # Shapes of data variables\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5f8de920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(output_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "12a76546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "10a4a9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.60392157 0.69411765 0.73333333]\n",
      "  [0.49411765 0.5372549  0.53333333]\n",
      "  [0.41176471 0.40784314 0.37254902]\n",
      "  ...\n",
      "  [0.35686275 0.37254902 0.27843137]\n",
      "  [0.34117647 0.35294118 0.27843137]\n",
      "  [0.30980392 0.31764706 0.2745098 ]]\n",
      "\n",
      " [[0.54901961 0.62745098 0.6627451 ]\n",
      "  [0.56862745 0.6        0.60392157]\n",
      "  [0.49019608 0.49019608 0.4627451 ]\n",
      "  ...\n",
      "  [0.37647059 0.38823529 0.30588235]\n",
      "  [0.30196078 0.31372549 0.24313725]\n",
      "  [0.27843137 0.28627451 0.23921569]]\n",
      "\n",
      " [[0.54901961 0.60784314 0.64313725]\n",
      "  [0.54509804 0.57254902 0.58431373]\n",
      "  [0.45098039 0.45098039 0.43921569]\n",
      "  ...\n",
      "  [0.30980392 0.32156863 0.25098039]\n",
      "  [0.26666667 0.2745098  0.21568627]\n",
      "  [0.2627451  0.27058824 0.21568627]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.68627451 0.65490196 0.65098039]\n",
      "  [0.61176471 0.60392157 0.62745098]\n",
      "  [0.60392157 0.62745098 0.66666667]\n",
      "  ...\n",
      "  [0.16470588 0.13333333 0.14117647]\n",
      "  [0.23921569 0.20784314 0.22352941]\n",
      "  [0.36470588 0.3254902  0.35686275]]\n",
      "\n",
      " [[0.64705882 0.60392157 0.50196078]\n",
      "  [0.61176471 0.59607843 0.50980392]\n",
      "  [0.62352941 0.63137255 0.55686275]\n",
      "  ...\n",
      "  [0.40392157 0.36470588 0.37647059]\n",
      "  [0.48235294 0.44705882 0.47058824]\n",
      "  [0.51372549 0.4745098  0.51372549]]\n",
      "\n",
      " [[0.63921569 0.58039216 0.47058824]\n",
      "  [0.61960784 0.58039216 0.47843137]\n",
      "  [0.63921569 0.61176471 0.52156863]\n",
      "  ...\n",
      "  [0.56078431 0.52156863 0.54509804]\n",
      "  [0.56078431 0.5254902  0.55686275]\n",
      "  [0.56078431 0.52156863 0.56470588]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2d6aec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# Create callbacks\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f\"conv_weights/Conv2D Model/{epochs}e-{batch_size}bs\" + \"-{loss:.4f}\" + \".keras\", \n",
    "    monitor='loss', \n",
    "    verbose=1,\n",
    "    save_best_only=True, mode='min')\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=3,          # Number of epochs with no improvement after which training stops\n",
    "    min_delta=0.001,     # Minimum change to qualify as an improvement\n",
    "    mode='min',          # The monitored quantity should be minimized (e.g., loss)\n",
    "    verbose=1,           # Log when the callback takes action\n",
    "    restore_best_weights=True # Restores model weights from the best epoch\n",
    ")\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cc0cee65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_55\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_55\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_215 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_216 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_101               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_217 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,030</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_102               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_218 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,530</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">750</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,530</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">310</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_215 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m100\u001b[0m)    │         \u001b[38;5;34m2,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_216 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m40\u001b[0m)     │        \u001b[38;5;34m36,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_101               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m40\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_217 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m30\u001b[0m)     │        \u001b[38;5;34m30,030\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_102               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m30\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_218 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m30\u001b[0m)       │        \u001b[38;5;34m22,530\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_55 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m750\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_98 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │        \u001b[38;5;34m22,530\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_99 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m310\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">114,240</span> (446.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m114,240\u001b[0m (446.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">114,240</span> (446.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m114,240\u001b[0m (446.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#Model Creation\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=100, kernel_size=(3,3), activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Conv2D(filters=40, kernel_size=(3,3), activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=30, kernel_size=(5,5), activation=tf.keras.activations.relu),\n",
    "\n",
    "    # tf.keras.layers.Conv2D(filters=40, kernel_size=(5,5), activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=30, kernel_size=(5,5), padding='same', activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=30, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax)\n",
    "])\n",
    "model.build(input_shape=(None,) + X_train.shape[1:4])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "58f4f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.2682 - loss: 1.9550\n",
      "Epoch 1: loss improved from None to 1.72868, saving model to conv_weights/Conv2D Model/10e-100bs-1.7287.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 97ms/step - accuracy: 0.3634 - loss: 1.7287 - val_accuracy: 0.4996 - val_loss: 1.3990\n",
      "Epoch 2/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5159 - loss: 1.3538\n",
      "Epoch 2: loss improved from 1.72868 to 1.30940, saving model to conv_weights/Conv2D Model/10e-100bs-1.3094.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 85ms/step - accuracy: 0.5325 - loss: 1.3094 - val_accuracy: 0.5538 - val_loss: 1.2472\n",
      "Epoch 3/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.5891 - loss: 1.1609\n",
      "Epoch 3: loss improved from 1.30940 to 1.13324, saving model to conv_weights/Conv2D Model/10e-100bs-1.1332.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - accuracy: 0.5988 - loss: 1.1332 - val_accuracy: 0.6288 - val_loss: 1.0739\n",
      "Epoch 4/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6383 - loss: 1.0308\n",
      "Epoch 4: loss improved from 1.13324 to 1.01682, saving model to conv_weights/Conv2D Model/10e-100bs-1.0168.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 88ms/step - accuracy: 0.6414 - loss: 1.0168 - val_accuracy: 0.6510 - val_loss: 1.0145\n",
      "Epoch 5/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.6713 - loss: 0.9382\n",
      "Epoch 5: loss improved from 1.01682 to 0.92996, saving model to conv_weights/Conv2D Model/10e-100bs-0.9300.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 167ms/step - accuracy: 0.6729 - loss: 0.9300 - val_accuracy: 0.6656 - val_loss: 0.9596\n",
      "Epoch 6/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.6999 - loss: 0.8660\n",
      "Epoch 6: loss improved from 0.92996 to 0.86163, saving model to conv_weights/Conv2D Model/10e-100bs-0.8616.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 213ms/step - accuracy: 0.6986 - loss: 0.8616 - val_accuracy: 0.6788 - val_loss: 0.9250\n",
      "Epoch 7/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.7201 - loss: 0.8087\n",
      "Epoch 7: loss improved from 0.86163 to 0.80584, saving model to conv_weights/Conv2D Model/10e-100bs-0.8058.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 229ms/step - accuracy: 0.7183 - loss: 0.8058 - val_accuracy: 0.6952 - val_loss: 0.8882\n",
      "Epoch 8/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.7333 - loss: 0.7663\n",
      "Epoch 8: loss improved from 0.80584 to 0.76217, saving model to conv_weights/Conv2D Model/10e-100bs-0.7622.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 242ms/step - accuracy: 0.7326 - loss: 0.7622 - val_accuracy: 0.7034 - val_loss: 0.8733\n",
      "Epoch 9/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.7456 - loss: 0.7310\n",
      "Epoch 9: loss improved from 0.76217 to 0.72379, saving model to conv_weights/Conv2D Model/10e-100bs-0.7238.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 234ms/step - accuracy: 0.7456 - loss: 0.7238 - val_accuracy: 0.7090 - val_loss: 0.8656\n",
      "Epoch 10/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.7588 - loss: 0.6942\n",
      "Epoch 10: loss improved from 0.72379 to 0.68699, saving model to conv_weights/Conv2D Model/10e-100bs-0.6870.keras\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 222ms/step - accuracy: 0.7593 - loss: 0.6870 - val_accuracy: 0.7098 - val_loss: 0.8745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3a37adff0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "# model.load_weights(\"conv_weights/Conv2D Model/20e-100bs-0.9729.keras\")\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, shuffle=False, callbacks=callbacks, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_train[:10])\n",
    "pred_decode = output_encoder.inverse_transform(predictions)\n",
    "print(pred_decode)\n",
    "print(output_encoder.inverse_transform(y_train[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/0_/nmh5w64x3q5_ffwfkr35sscw0000gn/T/tmphi4ywg6h'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='keras_tensor_43')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  4895227984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895229136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895229712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895229904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895230096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895230480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895230288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895230864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895230672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  4895231248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1766688857.598078  212361 tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "W0000 00:00:1766688857.598484  212361 tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "I0000 00:00:1766688857.608609  212361 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model) # or .from_saved_model(saved_model_dir)\n",
    "# tflite_model = converter.convert()\n",
    "# with open('model.tflite', 'wb') as f:\n",
    "#     f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbfe5d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a .h5 file\n",
    "model.save(\"object_classifier_model4.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b50ea4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_names = ['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f76c8b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow version 2.20.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767062541.593652  261481 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1767062541.595610  261481 single_machine.cc:376] Starting new session\n",
      "I0000 00:00:1767062541.630193  261481 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1767062541.630246  261481 single_machine.cc:376] Starting new session\n",
      "I0000 00:00:1767062541.661509  261481 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1767062541.661555  261481 single_machine.cc:376] Starting new session\n",
      "I0000 00:00:1767062541.681517  261481 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1767062541.681568  261481 single_machine.cc:376] Starting new session\n",
      "Running TensorFlow Graph Passes:   0%|          | 0/6 [00:00<?, ? passes/s]I0000 00:00:1767062542.122092  261481 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 27.86 passes/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input (input_image) provided is not found in given tensorflow graph. Placeholders in graph are: ['keras_tensor_402']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# # Convert the model\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# mlmodel = ct.convert(\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#     'object_classifier_model4.h5',\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#     source=\"tensorflow\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     12\u001b[39m image_input = ct.converters.mil.input_types.ImageType(\n\u001b[32m     13\u001b[39m     name=\u001b[33m'\u001b[39m\u001b[33minput_image\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     14\u001b[39m     shape=(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m), \n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     color_layout=\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m mlmodel = \u001b[43mct\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                     \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                     \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmlprogram\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mminimum_deployment_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mct\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43miOS16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# mlmodel = ct.convert(model=model, \u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#                      inputs=[ct.TensorType(dtype=np.float16)], \u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m#                      source=\"tensorflow\", convert_to=\"mlprogram\", \u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#                      minimum_deployment_target=ct.target.iOS16,\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#                      output_names=['output'])\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Save the converted model\u001b[39;00m\n\u001b[32m     31\u001b[39m mlmodel.save(\u001b[33m\"\u001b[39m\u001b[33mObjectClassifierModel.mlpackage\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/_converters_entry.py:646\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline, states)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(states) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m exact_source != \u001b[33m\"\u001b[39m\u001b[33mpytorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mstates\u001b[39m\u001b[33m'\u001b[39m\u001b[33m can only be passed with pytorch source model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m mlmodel = \u001b[43mmil_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexact_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexact_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs_as_tensor_or_image_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# None or list[ct.ImageType/ct.TensorType]\u001b[39;49;00m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclassifier_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclassifier_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_model_load\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_model_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpackage_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpackage_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecification_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspecification_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmain_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpass_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_default_fp16_io\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_default_fp16_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exact_target == \u001b[33m\"\u001b[39m\u001b[33mmlprogram\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mlmodel._input_has_infinite_upper_bound():\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    665\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFor mlprogram, inputs with infinite upper_bound is not allowed. Please set upper_bound\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    666\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m to a positive value in \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRangeDim()\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m for the \u001b[39m\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m param in ct.convert().\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    667\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/converter.py:186\u001b[39m, in \u001b[36mmil_convert\u001b[39m\u001b[34m(model, convert_from, convert_to, compute_units, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;129m@_profile\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmil_convert\u001b[39m(\n\u001b[32m    149\u001b[39m     model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     **kwargs\n\u001b[32m    154\u001b[39m ):\n\u001b[32m    155\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[33;03m    Convert model from a specified frontend `convert_from` to a specified\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03m    converter backend `convert_to`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m \u001b[33;03m        See `coremltools.converters.convert`\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mil_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mConverterRegistry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mct\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMLModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/converter.py:218\u001b[39m, in \u001b[36m_mil_convert\u001b[39m\u001b[34m(model, convert_from, convert_to, registry, modelClass, compute_units, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m     weights_dir = _tempfile.TemporaryDirectory()\n\u001b[32m    216\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mweights_dir\u001b[39m\u001b[33m\"\u001b[39m] = weights_dir.name\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m proto, mil_program = \u001b[43mmil_convert_to_proto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m _reset_conversion_state()\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to == \u001b[33m'\u001b[39m\u001b[33mmilinternal\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/converter.py:294\u001b[39m, in \u001b[36mmil_convert_to_proto\u001b[39m\u001b[34m(model, convert_from, convert_to, converter_registry, main_pipeline, **kwargs)\u001b[39m\n\u001b[32m    289\u001b[39m frontend_pipeline, backend_pipeline = _construct_other_pipelines(\n\u001b[32m    290\u001b[39m     main_pipeline, convert_from, convert_to\n\u001b[32m    291\u001b[39m )\n\u001b[32m    293\u001b[39m frontend_converter = frontend_converter_type()\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m prog = \u001b[43mfrontend_converter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m PassPipelineManager.apply_pipeline(prog, frontend_pipeline)\n\u001b[32m    297\u001b[39m PassPipelineManager.apply_pipeline(prog, main_pipeline)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/converter.py:96\u001b[39m, in \u001b[36mTensorFlow2Frontend.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfrontend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensorflow2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TF2Loader\n\u001b[32m     95\u001b[39m tf2_loader = TF2Loader(*args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf2_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/frontend/tensorflow/load.py:82\u001b[39m, in \u001b[36mTFLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m     dot_string = \u001b[38;5;28mself\u001b[39m._tf_ssa.get_dot_string(\n\u001b[32m     76\u001b[39m         annotation=\u001b[38;5;28;01mTrue\u001b[39;00m, name_and_op_style=\u001b[38;5;28;01mTrue\u001b[39;00m, highlight_debug_nodes=[]\n\u001b[32m     77\u001b[39m     )\n\u001b[32m     78\u001b[39m     graphviz.Source(dot_string).view(\n\u001b[32m     79\u001b[39m         filename=\u001b[33m\"\u001b[39m\u001b[33m/tmp/ssa_before_tf_passes\u001b[39m\u001b[33m\"\u001b[39m, cleanup=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     80\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m program = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_program_from_tf_ssa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mprogram:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(program))\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m program\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/frontend/tensorflow2/load.py:203\u001b[39m, in \u001b[36mTF2Loader._program_from_tf_ssa\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_program_from_tf_ssa\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    202\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_tf_ssa_passes()\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     converter = \u001b[43mTF2Converter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtfssa\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tf_ssa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspecification_version\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_default_fp16_io\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_default_fp16_io\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m converter.convert()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Projects/ML Models/Object Classifier/.venv/lib/python3.12/site-packages/coremltools/converters/mil/frontend/tensorflow/converter.py:196\u001b[39m, in \u001b[36mTFConverter.__init__\u001b[39m\u001b[34m(self, tfssa, inputs, outputs, opset_version, use_default_fp16_io)\u001b[39m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    193\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMultiple inputs are found in graph, but no input name was provided\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    194\u001b[39m     )\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inp.name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tf_placeholder_names:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInput (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m) provided is not found in given tensorflow graph. Placeholders in graph are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    198\u001b[39m             inp.name, tf_placeholder_names\n\u001b[32m    199\u001b[39m         )\n\u001b[32m    200\u001b[39m     )\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# We fill in shapes and dtypes for user-specified input that doesn't set\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inp.shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Input (input_image) provided is not found in given tensorflow graph. Placeholders in graph are: ['keras_tensor_402']"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "\n",
    "# # Convert the model\n",
    "# mlmodel = ct.convert(\n",
    "#     'object_classifier_model4.h5',\n",
    "#     convert_to=\"mlprogram\", # Use mlprogram for better performance on newer iOS versions\n",
    "#     minimum_deployment_target=ct.target.iOS15, # Specify your minimum iOS target\n",
    "#     source=\"tensorflow\"\n",
    "# )\n",
    "\n",
    "image_input = ct.converters.mil.input_types.ImageType(\n",
    "    name='input_image', \n",
    "    shape=(1, 3, 224, 224), \n",
    "    scale=1.0/255.0, \n",
    "    bias=[0.0, 0.0, 0.0], \n",
    "    color_layout='RGB'\n",
    ")\n",
    "\n",
    "mlmodel = ct.converters.convert(model=model, \n",
    "                     inputs=[image_input], \n",
    "                     source=\"auto\", convert_to=\"mlprogram\", \n",
    "                     minimum_deployment_target=ct.target.iOS16)\n",
    "\n",
    "# mlmodel = ct.convert(model=model, \n",
    "#                      inputs=[ct.TensorType(dtype=np.float16)], \n",
    "#                      source=\"tensorflow\", convert_to=\"mlprogram\", \n",
    "#                      minimum_deployment_target=ct.target.iOS16,\n",
    "#                      output_names=['output'])\n",
    "# Save the converted model\n",
    "mlmodel.save(\"ObjectClassifierModel.mlpackage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
